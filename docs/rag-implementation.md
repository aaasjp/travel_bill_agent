# RAGåŠŸèƒ½å®ç°æ–¹æ¡ˆ

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†åœ¨å·®æ—…æŠ¥é”€æ™ºèƒ½ä½“ä¸­é›†æˆRAGï¼ˆRetrieval-Augmented Generationï¼‰åŠŸèƒ½çš„å®Œæ•´å®ç°æ–¹æ¡ˆã€‚RAGç³»ç»Ÿå°†ä¸ºæ™ºèƒ½ä½“æä¾›åŸºäºçŸ¥è¯†åº“çš„ä¸Šä¸‹æ–‡å¢å¼ºèƒ½åŠ›ï¼Œæ˜¾è‘—æå‡å›ç­”å‡†ç¡®æ€§å’Œæ”¿ç­–åˆè§„æ€§ã€‚

## ğŸ¯ å®ç°ç›®æ ‡

### æ ¸å¿ƒç›®æ ‡
1. **çŸ¥è¯†å¢å¼º**ï¼šä¸ºLLMæä¾›å‡†ç¡®çš„å·®æ—…æ”¿ç­–å’Œæµç¨‹çŸ¥è¯†
2. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**ï¼šæ ¹æ®ç”¨æˆ·æŸ¥è¯¢åŠ¨æ€æ£€ç´¢ç›¸å…³ä¿¡æ¯
3. **åˆè§„ä¿éšœ**ï¼šç¡®ä¿å»ºè®®ç¬¦åˆæœ€æ–°çš„ä¼ä¸šæ”¿ç­–
4. **æ— ç¼é›†æˆ**ï¼šä¸ç°æœ‰LangGraphå·¥ä½œæµæ·±åº¦èåˆ

### ä¸šåŠ¡ä»·å€¼
- æé«˜å›ç­”å‡†ç¡®æ€§ä»70%æå‡è‡³95%
- å‡å°‘æ”¿ç­–è¿è§„é£é™©80%
- æå‡ç”¨æˆ·ä½“éªŒå’Œæ»¡æ„åº¦
- é™ä½äººå·¥å®¡æ ¸æˆæœ¬60%

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„å›¾
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAGå¢å¼ºå·¥ä½œæµ                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç”¨æˆ·è¾“å…¥ â†’ æ„å›¾åˆ†æ â†’ RAGæ£€ç´¢ â†’ ä»»åŠ¡è§„åˆ’ â†’ å·¥å…·æ‰§è¡Œ â†’ ç»“æœè¾“å‡º    â”‚
â”‚              â†“         â†“         â†“                          â”‚
â”‚          çŸ¥è¯†å¢å¼º   ä¸Šä¸‹æ–‡æ³¨å…¥   æ”¿ç­–éªŒè¯                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RAGæ ¸å¿ƒç»„ä»¶æ¶æ„                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ–‡æ¡£åŠ è½½å™¨ â†’ æ–‡æœ¬åˆ†å‰² â†’ å‘é‡åŒ– â†’ å‘é‡æ•°æ®åº“                   â”‚
â”‚      â†“                                â†“                     â”‚
â”‚  çŸ¥è¯†åº“ç®¡ç† â† â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ è¯­ä¹‰æ£€ç´¢                   â”‚
â”‚      â†“                                â†“                     â”‚
â”‚  ä¸Šä¸‹æ–‡å¢å¼º â†’ æç¤ºä¼˜åŒ– â†’ LLMè°ƒç”¨ â†’ ç»“æœéªŒè¯                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶
1. **çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ** (Knowledge Management)
2. **æ–‡æ¡£å¤„ç†å¼•æ“** (Document Processing)
3. **å‘é‡æ£€ç´¢ç³»ç»Ÿ** (Vector Retrieval)
4. **ä¸Šä¸‹æ–‡å¢å¼ºå™¨** (Context Augmentor)
5. **RAGé›†æˆèŠ‚ç‚¹** (RAG Integration Nodes)

## ğŸ“ æ–‡ä»¶ç»“æ„è®¾è®¡

```
src/
â”œâ”€â”€ rag/                           # RAGæ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ knowledge/                 # çŸ¥è¯†ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loader.py             # æ–‡æ¡£åŠ è½½å™¨
â”‚   â”‚   â”œâ”€â”€ splitter.py           # æ–‡æœ¬åˆ†å‰²å™¨
â”‚   â”‚   â”œâ”€â”€ embedder.py           # å‘é‡åŒ–å™¨
â”‚   â”‚   â””â”€â”€ manager.py            # çŸ¥è¯†åº“ç®¡ç†å™¨
â”‚   â”œâ”€â”€ retrieval/                # æ£€ç´¢ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ retriever.py          # æ£€ç´¢å™¨
â”‚   â”‚   â”œâ”€â”€ reranker.py           # é‡æ’åºå™¨
â”‚   â”‚   â””â”€â”€ filters.py            # è¿‡æ»¤å™¨
â”‚   â”œâ”€â”€ enhancement/              # å¢å¼ºç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ augmentor.py          # ä¸Šä¸‹æ–‡å¢å¼ºå™¨
â”‚   â”‚   â”œâ”€â”€ validator.py          # ç»“æœéªŒè¯å™¨
â”‚   â”‚   â””â”€â”€ optimizer.py          # æç¤ºä¼˜åŒ–å™¨
â”‚   â””â”€â”€ integration/              # é›†æˆç»„ä»¶
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ rag_node.py          # RAGèŠ‚ç‚¹
â”‚       â””â”€â”€ rag_tools.py         # RAGå·¥å…·
â”œâ”€â”€ nodes/                        # ç°æœ‰èŠ‚ç‚¹ï¼ˆéœ€è¦æ”¹é€ ï¼‰
â”‚   â”œâ”€â”€ intent_analysis.py       # é›†æˆRAGçš„æ„å›¾åˆ†æ
â”‚   â”œâ”€â”€ task_planning.py         # é›†æˆRAGçš„ä»»åŠ¡è§„åˆ’
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/                        # æ•°æ®ç›®å½•ï¼ˆæ–°å¢ï¼‰
â”‚   â”œâ”€â”€ knowledge_base/          # çŸ¥è¯†åº“æ–‡æ¡£
â”‚   â”‚   â”œâ”€â”€ policies/           # æ”¿ç­–æ–‡æ¡£
â”‚   â”‚   â”œâ”€â”€ procedures/         # æµç¨‹æ–‡æ¡£
â”‚   â”‚   â”œâ”€â”€ faqs/              # å¸¸è§é—®é¢˜
â”‚   â”‚   â””â”€â”€ templates/         # æ¨¡æ¿æ–‡æ¡£
â”‚   â””â”€â”€ vector_store/           # å‘é‡æ•°æ®åº“
â”‚       â””â”€â”€ chroma_db/         # ChromaDBå­˜å‚¨
â””â”€â”€ config/
    â””â”€â”€ rag_config.py           # RAGé…ç½®æ–‡ä»¶
```

## ğŸ”§ æŠ€æœ¯å®ç°ç»†èŠ‚

### 1. ä¾èµ–æ›´æ–° (requirements.txt)

```python
# ç°æœ‰ä¾èµ–ä¿æŒä¸å˜...

# RAGç›¸å…³æ–°å¢ä¾èµ–
chromadb>=0.4.15              # å‘é‡æ•°æ®åº“
sentence-transformers>=2.2.2  # å¥å­åµŒå…¥
langchain-community>=0.0.13   # æ–‡æ¡£åŠ è½½å™¨
pypdf>=3.17.0                 # PDFå¤„ç†
python-docx>=1.1.0           # Wordæ–‡æ¡£å¤„ç†
openpyxl>=3.1.2              # Excelå¤„ç†
markdown>=3.5.1              # Markdownå¤„ç†
jieba>=0.42.1                # ä¸­æ–‡åˆ†è¯
rank-bm25>=0.2.2             # BM25æ£€ç´¢
faiss-cpu>=1.7.4             # FAISSå‘é‡æ£€ç´¢ï¼ˆå¯é€‰ï¼‰
```

### 2. RAGé…ç½®æ–‡ä»¶ (src/config/rag_config.py)

```python
import os
from typing import Dict, Any
from dataclasses import dataclass

@dataclass
class RAGConfig:
    """RAGç³»ç»Ÿé…ç½®"""
    
    # å‘é‡æ•°æ®åº“é…ç½®
    VECTOR_DB_TYPE: str = "chroma"  # chroma, faiss
    VECTOR_DB_PATH: str = "data/vector_store/chroma_db"
    COLLECTION_NAME: str = "expense_knowledge"
    
    # åµŒå…¥æ¨¡å‹é…ç½®
    EMBEDDING_MODEL: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    EMBEDDING_DIMENSION: int = 384
    
    # æ–‡æœ¬åˆ†å‰²é…ç½®
    CHUNK_SIZE: int = 1000
    CHUNK_OVERLAP: int = 200
    
    # æ£€ç´¢é…ç½®
    TOP_K: int = 5
    SIMILARITY_THRESHOLD: float = 0.7
    RERANK_TOP_K: int = 3
    
    # çŸ¥è¯†åº“è·¯å¾„
    KNOWLEDGE_BASE_PATH: str = "data/knowledge_base"
    POLICIES_PATH: str = "data/knowledge_base/policies"
    PROCEDURES_PATH: str = "data/knowledge_base/procedures"
    FAQS_PATH: str = "data/knowledge_base/faqs"
    
    # æ£€ç´¢ç­–ç•¥
    RETRIEVAL_STRATEGIES: Dict[str, Any] = {
        "semantic": {"weight": 0.7},  # è¯­ä¹‰æ£€ç´¢æƒé‡
        "keyword": {"weight": 0.3},   # å…³é”®è¯æ£€ç´¢æƒé‡
        "hybrid": True                # æ˜¯å¦å¯ç”¨æ··åˆæ£€ç´¢
    }
    
    # ç¼“å­˜é…ç½®
    ENABLE_CACHE: bool = True
    CACHE_TTL: int = 3600  # ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)

# å…¨å±€é…ç½®å®ä¾‹
rag_config = RAGConfig()
```

### 3. çŸ¥è¯†åº“ç®¡ç†å™¨ (src/rag/knowledge/manager.py)

```python
import os
import json
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
import chromadb
from chromadb.config import Settings

from .loader import DocumentLoader
from .splitter import TextSplitter
from .embedder import EmbeddingService
from ...config.rag_config import rag_config

logger = logging.getLogger(__name__)

class KnowledgeManager:
    """çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“ç®¡ç†å™¨"""
        self.config = rag_config
        self.client = None
        self.collection = None
        self.loader = DocumentLoader()
        self.splitter = TextSplitter()
        self.embedder = EmbeddingService()
        
        self._initialize_vector_db()
    
    def _initialize_vector_db(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        try:
            # åˆ›å»ºChromaDBå®¢æˆ·ç«¯
            persist_directory = Path(self.config.VECTOR_DB_PATH)
            persist_directory.mkdir(parents=True, exist_ok=True)
            
            self.client = chromadb.PersistentClient(
                path=str(persist_directory),
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # è·å–æˆ–åˆ›å»ºé›†åˆ
            try:
                self.collection = self.client.get_collection(
                    name=self.config.COLLECTION_NAME
                )
                logger.info(f"æˆåŠŸè¿æ¥åˆ°ç°æœ‰é›†åˆ: {self.config.COLLECTION_NAME}")
            except:
                self.collection = self.client.create_collection(
                    name=self.config.COLLECTION_NAME,
                    metadata={"description": "å·®æ—…æŠ¥é”€çŸ¥è¯†åº“"}
                )
                logger.info(f"åˆ›å»ºæ–°é›†åˆ: {self.config.COLLECTION_NAME}")
                
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
            raise
    
    async def build_knowledge_base(self, force_rebuild: bool = False):
        """æ„å»ºçŸ¥è¯†åº“"""
        logger.info("å¼€å§‹æ„å»ºçŸ¥è¯†åº“...")
        
        if force_rebuild:
            await self._clear_knowledge_base()
        
        # åŠ è½½å„ç±»æ–‡æ¡£
        knowledge_paths = [
            (self.config.POLICIES_PATH, "policy"),
            (self.config.PROCEDURES_PATH, "procedure"), 
            (self.config.FAQS_PATH, "faq")
        ]
        
        total_chunks = 0
        for path, doc_type in knowledge_paths:
            if os.path.exists(path):
                chunks = await self._process_directory(path, doc_type)
                total_chunks += len(chunks)
                logger.info(f"å¤„ç† {doc_type} æ–‡æ¡£: {len(chunks)} ä¸ªç‰‡æ®µ")
        
        logger.info(f"çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œå…± {total_chunks} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        return total_chunks
    
    async def _process_directory(self, directory_path: str, doc_type: str) -> List[Dict[str, Any]]:
        """å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£"""
        chunks = []
        directory = Path(directory_path)
        
        if not directory.exists():
            logger.warning(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")
            return chunks
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        supported_extensions = {'.md', '.txt', '.pdf', '.docx', '.xlsx'}
        
        for file_path in directory.rglob('*'):
            if file_path.suffix.lower() in supported_extensions:
                try:
                    # åŠ è½½æ–‡æ¡£
                    documents = await self.loader.load_document(str(file_path))
                    
                    for doc in documents:
                        # åˆ†å‰²æ–‡æœ¬
                        text_chunks = self.splitter.split_text(doc.content)
                        
                        # ç”Ÿæˆå‘é‡å¹¶å­˜å‚¨
                        for i, chunk in enumerate(text_chunks):
                            chunk_data = {
                                "id": f"{file_path.stem}_{i}",
                                "content": chunk,
                                "metadata": {
                                    "source": str(file_path),
                                    "doc_type": doc_type,
                                    "title": doc.metadata.get("title", file_path.stem),
                                    "chunk_index": i,
                                    "total_chunks": len(text_chunks)
                                }
                            }
                            chunks.append(chunk_data)
                            
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
                    continue
        
        # æ‰¹é‡å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        if chunks:
            await self._store_chunks(chunks)
        
        return chunks
    
    async def _store_chunks(self, chunks: List[Dict[str, Any]]):
        """æ‰¹é‡å­˜å‚¨æ–‡æ¡£ç‰‡æ®µåˆ°å‘é‡æ•°æ®åº“"""
        try:
            # æå–æ–‡æœ¬å†…å®¹
            texts = [chunk["content"] for chunk in chunks]
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            embeddings = await self.embedder.embed_texts(texts)
            
            # å‡†å¤‡å­˜å‚¨æ•°æ®
            ids = [chunk["id"] for chunk in chunks]
            metadatas = [chunk["metadata"] for chunk in chunks]
            
            # å­˜å‚¨åˆ°ChromaDB
            self.collection.add(
                embeddings=embeddings,
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
            
            logger.info(f"æˆåŠŸå­˜å‚¨ {len(chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            
        except Exception as e:
            logger.error(f"å­˜å‚¨æ–‡æ¡£ç‰‡æ®µå¤±è´¥: {str(e)}")
            raise
    
    async def _clear_knowledge_base(self):
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        try:
            self.client.delete_collection(self.config.COLLECTION_NAME)
            self.collection = self.client.create_collection(
                name=self.config.COLLECTION_NAME,
                metadata={"description": "å·®æ—…æŠ¥é”€çŸ¥è¯†åº“"}
            )
            logger.info("çŸ¥è¯†åº“å·²æ¸…ç©º")
        except Exception as e:
            logger.error(f"æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}")
            raise
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            count = self.collection.count()
            return {
                "total_documents": count,
                "collection_name": self.config.COLLECTION_NAME,
                "embedding_model": self.config.EMBEDDING_MODEL
            }
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}
```

### 4. æ£€ç´¢å™¨ (src/rag/retrieval/retriever.py)

```python
import logging
from typing import List, Dict, Any, Optional, Tuple
import asyncio
from rank_bm25 import BM25Okapi
import jieba
import numpy as np

from ..knowledge.manager import KnowledgeManager
from ...config.rag_config import rag_config

logger = logging.getLogger(__name__)

class RetrievalResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç±»"""
    def __init__(self, content: str, metadata: Dict[str, Any], score: float):
        self.content = content
        self.metadata = metadata
        self.score = score
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "content": self.content,
            "metadata": self.metadata,
            "score": self.score
        }

class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨ - ç»“åˆè¯­ä¹‰æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢"""
    
    def __init__(self, knowledge_manager: KnowledgeManager):
        self.knowledge_manager = knowledge_manager
        self.config = rag_config
        self.bm25_index = None
        self.bm25_docs = []
        self.bm25_metadata = []
        
        # åˆå§‹åŒ–BM25ç´¢å¼•
        asyncio.create_task(self._build_bm25_index())
    
    async def _build_bm25_index(self):
        """æ„å»ºBM25ç´¢å¼•"""
        try:
            # ä»å‘é‡æ•°æ®åº“è·å–æ‰€æœ‰æ–‡æ¡£
            collection = self.knowledge_manager.collection
            results = collection.get()
            
            if not results['documents']:
                logger.warning("çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— æ³•æ„å»ºBM25ç´¢å¼•")
                return
            
            # åˆ†è¯å¤„ç†
            tokenized_docs = []
            for doc in results['documents']:
                tokens = list(jieba.cut(doc))
                tokenized_docs.append(tokens)
            
            # æ„å»ºBM25ç´¢å¼•
            self.bm25_index = BM25Okapi(tokenized_docs)
            self.bm25_docs = results['documents']
            self.bm25_metadata = results['metadatas']
            
            logger.info(f"BM25ç´¢å¼•æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(tokenized_docs)} ä¸ªæ–‡æ¡£")
            
        except Exception as e:
            logger.error(f"æ„å»ºBM25ç´¢å¼•å¤±è´¥: {str(e)}")
    
    async def retrieve(self, query: str, top_k: int = None) -> List[RetrievalResult]:
        """æ··åˆæ£€ç´¢"""
        if top_k is None:
            top_k = self.config.TOP_K
        
        # å¹¶è¡Œæ‰§è¡Œè¯­ä¹‰æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢
        semantic_results, keyword_results = await asyncio.gather(
            self._semantic_retrieve(query, top_k * 2),
            self._keyword_retrieve(query, top_k * 2)
        )
        
        # èåˆç»“æœ
        fused_results = self._fuse_results(semantic_results, keyword_results)
        
        # è¿”å›Top-Kç»“æœ
        return fused_results[:top_k]
    
    async def _semantic_retrieve(self, query: str, top_k: int) -> List[RetrievalResult]:
        """è¯­ä¹‰æ£€ç´¢"""
        try:
            collection = self.knowledge_manager.collection
            
            # æŸ¥è¯¢å‘é‡åŒ–
            query_embedding = await self.knowledge_manager.embedder.embed_texts([query])
            
            # å‘é‡æ£€ç´¢
            results = collection.query(
                query_embeddings=query_embedding,
                n_results=top_k,
                include=['documents', 'metadatas', 'distances']
            )
            
            # è½¬æ¢ä¸ºRetrievalResult
            retrieval_results = []
            for i, (doc, metadata, distance) in enumerate(zip(
                results['documents'][0],
                results['metadatas'][0], 
                results['distances'][0]
            )):
                # å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                similarity_score = 1 / (1 + distance)
                
                if similarity_score >= self.config.SIMILARITY_THRESHOLD:
                    retrieval_results.append(RetrievalResult(
                        content=doc,
                        metadata=metadata,
                        score=similarity_score
                    ))
            
            logger.debug(f"è¯­ä¹‰æ£€ç´¢è¿”å› {len(retrieval_results)} ä¸ªç»“æœ")
            return retrieval_results
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰æ£€ç´¢å¤±è´¥: {str(e)}")
            return []
    
    async def _keyword_retrieve(self, query: str, top_k: int) -> List[RetrievalResult]:
        """å…³é”®è¯æ£€ç´¢ï¼ˆBM25ï¼‰"""
        try:
            if not self.bm25_index:
                logger.warning("BM25ç´¢å¼•æœªæ„å»ºï¼Œè·³è¿‡å…³é”®è¯æ£€ç´¢")
                return []
            
            # æŸ¥è¯¢åˆ†è¯
            query_tokens = list(jieba.cut(query))
            
            # BM25æ£€ç´¢
            scores = self.bm25_index.get_scores(query_tokens)
            
            # è·å–Top-Kç»“æœ
            top_k_indices = np.argsort(scores)[::-1][:top_k]
            
            retrieval_results = []
            for idx in top_k_indices:
                if scores[idx] > 0:  # åªè¿”å›æœ‰å¾—åˆ†çš„ç»“æœ
                    retrieval_results.append(RetrievalResult(
                        content=self.bm25_docs[idx],
                        metadata=self.bm25_metadata[idx],
                        score=float(scores[idx])
                    ))
            
            logger.debug(f"å…³é”®è¯æ£€ç´¢è¿”å› {len(retrieval_results)} ä¸ªç»“æœ")
            return retrieval_results
            
        except Exception as e:
            logger.error(f"å…³é”®è¯æ£€ç´¢å¤±è´¥: {str(e)}")
            return []
    
    def _fuse_results(self, semantic_results: List[RetrievalResult], 
                     keyword_results: List[RetrievalResult]) -> List[RetrievalResult]:
        """èåˆè¯­ä¹‰æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢ç»“æœ"""
        
        # è·å–æƒé‡é…ç½®
        semantic_weight = self.config.RETRIEVAL_STRATEGIES["semantic"]["weight"]
        keyword_weight = self.config.RETRIEVAL_STRATEGIES["keyword"]["weight"]
        
        # ä½¿ç”¨æ–‡æ¡£IDå»é‡å¹¶èåˆåˆ†æ•°
        fused_dict = {}
        
        # å¤„ç†è¯­ä¹‰æ£€ç´¢ç»“æœ
        for result in semantic_results:
            doc_id = result.metadata.get("source", "") + str(hash(result.content))
            fused_dict[doc_id] = {
                "result": result,
                "semantic_score": result.score,
                "keyword_score": 0.0
            }
        
        # å¤„ç†å…³é”®è¯æ£€ç´¢ç»“æœ
        for result in keyword_results:
            doc_id = result.metadata.get("source", "") + str(hash(result.content))
            if doc_id in fused_dict:
                fused_dict[doc_id]["keyword_score"] = result.score
            else:
                fused_dict[doc_id] = {
                    "result": result,
                    "semantic_score": 0.0,
                    "keyword_score": result.score
                }
        
        # è®¡ç®—èåˆåˆ†æ•°å¹¶æ’åº
        fused_results = []
        for doc_id, data in fused_dict.items():
            # è®¡ç®—åŠ æƒåˆ†æ•°
            final_score = (
                data["semantic_score"] * semantic_weight + 
                data["keyword_score"] * keyword_weight
            )
            
            # æ›´æ–°ç»“æœåˆ†æ•°
            result = data["result"]
            result.score = final_score
            fused_results.append(result)
        
        # æŒ‰åˆ†æ•°é™åºæ’åˆ—
        fused_results.sort(key=lambda x: x.score, reverse=True)
        
        logger.debug(f"èåˆåè¿”å› {len(fused_results)} ä¸ªç»“æœ")
        return fused_results
```

### 5. RAGé›†æˆèŠ‚ç‚¹ (src/rag/integration/rag_node.py)

```python
import logging
from typing import Dict, Any, List, Optional
import asyncio

from ...models.state import ExpenseState
from ..knowledge.manager import KnowledgeManager
from ..retrieval.retriever import HybridRetriever
from ..enhancement.augmentor import ContextAugmentor
from ...config.rag_config import rag_config

logger = logging.getLogger(__name__)

class RAGNode:
    """RAGèŠ‚ç‚¹ - è´Ÿè´£çŸ¥è¯†æ£€ç´¢å’Œä¸Šä¸‹æ–‡å¢å¼º"""
    
    def __init__(self):
        self.config = rag_config
        self.knowledge_manager = KnowledgeManager()
        self.retriever = HybridRetriever(self.knowledge_manager)
        self.augmentor = ContextAugmentor()
        
    async def enhance_intent_analysis(self, state: ExpenseState) -> ExpenseState:
        """ä¸ºæ„å›¾åˆ†ææä¾›çŸ¥è¯†å¢å¼º"""
        try:
            user_input = state.get("user_input", "")
            if not user_input:
                return state
            
            # æ£€ç´¢ç›¸å…³çŸ¥è¯†
            relevant_docs = await self.retriever.retrieve(user_input)
            
            # æ„å»ºçŸ¥è¯†ä¸Šä¸‹æ–‡
            knowledge_context = self._build_knowledge_context(relevant_docs)
            
            # å¢å¼ºçŠ¶æ€
            enhanced_state = state.copy()
            enhanced_state["rag_context"] = {
                "knowledge_context": knowledge_context,
                "retrieved_docs": [doc.to_dict() for doc in relevant_docs],
                "retrieval_query": user_input
            }
            
            logger.info(f"ä¸ºæ„å›¾åˆ†ææ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            return enhanced_state
            
        except Exception as e:
            logger.error(f"æ„å›¾åˆ†æRAGå¢å¼ºå¤±è´¥: {str(e)}")
            return state
    
    async def enhance_task_planning(self, state: ExpenseState) -> ExpenseState:
        """ä¸ºä»»åŠ¡è§„åˆ’æä¾›çŸ¥è¯†å¢å¼º"""
        try:
            intent = state.get("intent", {})
            user_input = state.get("user_input", "")
            
            # æ„å»ºæ£€ç´¢æŸ¥è¯¢
            planning_query = self._build_planning_query(intent, user_input)
            
            # æ£€ç´¢æµç¨‹å’Œæ”¿ç­–ç›¸å…³çŸ¥è¯†
            relevant_docs = await self.retriever.retrieve(planning_query)
            
            # è¿‡æ»¤å‡ºæµç¨‹ç›¸å…³æ–‡æ¡£
            procedure_docs = [doc for doc in relevant_docs 
                            if doc.metadata.get("doc_type") == "procedure"]
            policy_docs = [doc for doc in relevant_docs 
                         if doc.metadata.get("doc_type") == "policy"]
            
            # æ„å»ºä»»åŠ¡è§„åˆ’ä¸Šä¸‹æ–‡
            planning_context = {
                "procedures": self._build_knowledge_context(procedure_docs),
                "policies": self._build_knowledge_context(policy_docs),
                "all_context": self._build_knowledge_context(relevant_docs)
            }
            
            # æ›´æ–°çŠ¶æ€
            enhanced_state = state.copy()
            if "rag_context" not in enhanced_state:
                enhanced_state["rag_context"] = {}
            
            enhanced_state["rag_context"]["planning_context"] = planning_context
            enhanced_state["rag_context"]["planning_docs"] = [doc.to_dict() for doc in relevant_docs]
            
            logger.info(f"ä¸ºä»»åŠ¡è§„åˆ’æ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            return enhanced_state
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡è§„åˆ’RAGå¢å¼ºå¤±è´¥: {str(e)}")
            return state
    
    def _build_knowledge_context(self, docs: List) -> str:
        """æ„å»ºçŸ¥è¯†ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        if not docs:
            return "æš‚æ— ç›¸å…³çŸ¥è¯†ã€‚"
        
        context_parts = []
        for i, doc in enumerate(docs[:self.config.RERANK_TOP_K], 1):
            source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
            title = doc.metadata.get("title", "æ— æ ‡é¢˜")
            content = doc.content[:500] + "..." if len(doc.content) > 500 else doc.content
            
            context_parts.append(f"""
{i}. ã€{title}ã€‘
æ¥æº: {source}
å†…å®¹: {content}
ç›¸å…³åº¦: {doc.score:.3f}
            """.strip())
        
        return "\n\n".join(context_parts)
    
    def _build_planning_query(self, intent: Dict[str, Any], user_input: str) -> str:
        """æ„å»ºä»»åŠ¡è§„åˆ’çš„æ£€ç´¢æŸ¥è¯¢"""
        intent_type = intent.get("ä¸»è¦æ„å›¾", "")
        details = intent.get("ç»†èŠ‚", {})
        
        # åŸºç¡€æŸ¥è¯¢
        query_parts = [user_input, intent_type]
        
        # æ·»åŠ ç»†èŠ‚ä¿¡æ¯
        for key, value in details.items():
            if isinstance(value, str) and value:
                query_parts.append(f"{key}:{value}")
        
        # æ·»åŠ ä»»åŠ¡è§„åˆ’ç›¸å…³å…³é”®è¯
        query_parts.extend(["æµç¨‹", "æ­¥éª¤", "è§„åˆ’", "è¦æ±‚"])
        
        return " ".join(query_parts)
```

## ğŸ”„ ç°æœ‰èŠ‚ç‚¹æ”¹é€ 

### 6. å¢å¼ºç‰ˆæ„å›¾åˆ†æèŠ‚ç‚¹

éœ€è¦ä¿®æ”¹ `src/nodes/intent_analysis.py`ï¼Œåœ¨å…³é”®ä½ç½®é›†æˆRAGåŠŸèƒ½ï¼š

```python
# åœ¨ IntentAnalysisNode ç±»ä¸­æ·»åŠ RAGé›†æˆ

from ..rag.integration.rag_node import RAGNode

class IntentAnalysisNode:
    def __init__(self, model_name: str = "qwen3-235b-a22b"):
        self.model_name = model_name
        self.rag_node = RAGNode()  # æ–°å¢RAGèŠ‚ç‚¹
    
    async def __call__(self, state: ExpenseState) -> ExpenseState:
        """å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œè¯†åˆ«æ„å›¾å’Œå·¥å…·è°ƒç”¨"""
        
        # 1. RAGå¢å¼º - è·å–ç›¸å…³çŸ¥è¯†ä¸Šä¸‹æ–‡
        enhanced_state = await self.rag_node.enhance_intent_analysis(state)
        
        # 2. æ„å»ºå¢å¼ºçš„ç³»ç»Ÿæç¤ºï¼ˆåŒ…å«çŸ¥è¯†ä¸Šä¸‹æ–‡ï¼‰
        system_prompt = self._build_enhanced_system_prompt(
            tools, 
            enhanced_state.get("rag_context", {}).get("knowledge_context", "")
        )
        
        # ... å…¶ä½™é€»è¾‘ä¿æŒä¸å˜
        
    def _build_enhanced_system_prompt(self, tools: List[Dict[str, Any]], knowledge_context: str) -> str:
        """æ„å»ºåŒ…å«RAGçŸ¥è¯†çš„å¢å¼ºç³»ç»Ÿæç¤º"""
        
        tools_desc = ""
        if tools:
            tools_desc = "å¯ç”¨å·¥å…·:\n"
            for i, tool in enumerate(tools, 1):
                tools_desc += f"{i}. {tool['name']}: {tool['description']}\n"
                tools_desc += f"   å‚æ•°: {json.dumps(tool['parameters'], ensure_ascii=False)}\n\n"
        
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·®æ—…æŠ¥é”€åŠ©æ‰‹ï¼Œè´Ÿè´£ç†è§£ç”¨æˆ·çš„æŠ¥é”€éœ€æ±‚å¹¶æä¾›å¸®åŠ©ã€‚

ã€ç›¸å…³çŸ¥è¯†å’Œæ”¿ç­–ã€‘:
{knowledge_context}

ã€å¯ç”¨å·¥å…·åˆ—è¡¨ã€‘:
{tools_desc}

è¯·åŸºäºä¸Šè¿°çŸ¥è¯†å’Œæ”¿ç­–ï¼Œåˆ†æç”¨æˆ·è¾“å…¥ï¼Œè¯†åˆ«ç”¨æˆ·çš„æ„å›¾å’Œéœ€è¦æ‰§è¡Œçš„æ“ä½œã€‚

å¦‚æœç”¨æˆ·è¯·æ±‚éœ€è¦ä½¿ç”¨å·¥å…·ï¼Œè¯·ç”Ÿæˆå·¥å…·è°ƒç”¨ã€‚å·¥å…·è°ƒç”¨æ ¼å¼å¦‚ä¸‹:
{{
  "id": "å·¥å…·è°ƒç”¨ID",
  "name": "å·¥å…·åç§°", 
  "arguments": {{
    "å‚æ•°1": "å€¼1",
    "å‚æ•°2": "å€¼2"
  }}
}}

å›å¤æ ¼å¼:
{{
  "intent": {{
    "ä¸»è¦æ„å›¾": "ç”¨æˆ·çš„ä¸»è¦æ„å›¾",
    "ç»†èŠ‚": {{
      "å…³é”®å­—æ®µ1": "å€¼1",
      "å…³é”®å­—æ®µ2": "å€¼2"
    }},
    "æ”¿ç­–ä¾æ®": "ç›¸å…³æ”¿ç­–æ¡æ¬¾",
    "åˆè§„æ€§æ£€æŸ¥": "æ˜¯å¦ç¬¦åˆæ”¿ç­–è¦æ±‚"
  }},
  "tool_calls": [
    {{
      "id": "å·¥å…·è°ƒç”¨ID",
      "name": "å·¥å…·åç§°",
      "arguments": {{
        "å‚æ•°1": "å€¼1",
        "å‚æ•°2": "å€¼2"
      }}
    }}
  ]
}}

å¦‚æœä¸éœ€è¦ä½¿ç”¨å·¥å…·ï¼Œåˆ™ä¸è¦åŒ…å«tool_callså­—æ®µã€‚
è¯·å§‹ç»ˆä»¥JSONæ ¼å¼è¿”å›ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®ã€‚"""
```

### 7. å¢å¼ºç‰ˆä»»åŠ¡è§„åˆ’èŠ‚ç‚¹

éœ€è¦ä¿®æ”¹ `src/nodes/task_planning.py`ï¼š

```python
# åœ¨ TaskPlanningNode ç±»ä¸­é›†æˆRAG

from ..rag.integration.rag_node import RAGNode

class TaskPlanningNode:
    def __init__(self, model_name: str = MODEL_NAME):
        self.model = ChatOpenAI(model_name=model_name, base_url=MODEL_BASE_URL, api_key=API_KEY)
        self.tool_registry = tool_registry
        self.rag_node = RAGNode()  # æ–°å¢RAGèŠ‚ç‚¹
        
        # æ›´æ–°æç¤ºæ¨¡æ¿ä»¥åŒ…å«RAGä¸Šä¸‹æ–‡
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·®æ—…æŠ¥é”€è§„åˆ’åŠ©æ‰‹ã€‚

ã€ä¸šåŠ¡æµç¨‹è¯´æ˜ã€‘:
{business_process_prompt}

ã€ç›¸å…³æ”¿ç­–çŸ¥è¯†ã€‘:
{rag_policies_context}

ã€æµç¨‹æŒ‡å¯¼ã€‘:
{rag_procedures_context}

ã€å¯ç”¨å·¥å…·åˆ—è¡¨ã€‘:
{available_tools}

åŸºäºä¸Šè¿°ä¿¡æ¯ï¼Œè¯·ä¸ºç”¨æˆ·ç”Ÿæˆè¯¦ç»†çš„æ‰§è¡Œè®¡åˆ’ã€‚

è¦æ±‚ï¼š
1. ä¸¥æ ¼éµå¾ªç›¸å…³æ”¿ç­–è¦æ±‚
2. æŒ‰ç…§æ ‡å‡†æµç¨‹è®¾è®¡æ­¥éª¤
3. ä¸ºæ¯ä¸ªæ­¥éª¤åˆ†é…é€‚å½“çš„å·¥å…·
4. ç¡®ä¿åˆè§„æ€§å’Œå®Œæ•´æ€§

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- steps: æ‰§è¡Œæ­¥éª¤åˆ—è¡¨
- tools_needed: æ¯ä¸ªæ­¥éª¤éœ€è¦çš„å·¥å…·
- dependencies: æ­¥éª¤é—´çš„ä¾èµ–å…³ç³»
- compliance_notes: åˆè§„æ€§è¯´æ˜
- estimated_time: é¢„è®¡å®Œæˆæ—¶é—´
            """),
            ("user", "æ„å›¾: {intent}\nä¸Šä¸‹æ–‡: {context}")
        ])
    
    async def __call__(self, state: ExpenseState) -> ExpenseState:
        """æ‰§è¡Œä»»åŠ¡è§„åˆ’"""
        
        # 1. RAGå¢å¼º - è·å–è§„åˆ’ç›¸å…³çŸ¥è¯†
        enhanced_state = await self.rag_node.enhance_task_planning(state)
        
        # 2. æå–RAGä¸Šä¸‹æ–‡
        rag_context = enhanced_state.get("rag_context", {})
        planning_context = rag_context.get("planning_context", {})
        
        # 3. å‡†å¤‡å¢å¼ºçš„è¾“å…¥
        tools_description = self._get_available_tools_description()
        
        inputs = {
            "intent": enhanced_state["intent"],
            "context": enhanced_state.get("context", {}),
            "business_process_prompt": business_process_prompt,
            "rag_policies_context": planning_context.get("policies", "æš‚æ— ç›¸å…³æ”¿ç­–"),
            "rag_procedures_context": planning_context.get("procedures", "æš‚æ— ç›¸å…³æµç¨‹æŒ‡å¯¼"),
            "available_tools": tools_description
        }
        
        # ... å…¶ä½™é€»è¾‘ä¿æŒä¸å˜ï¼Œä½†ä½¿ç”¨å¢å¼ºçš„è¾“å…¥
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§

### 8. ç¼“å­˜æœºåˆ¶ (src/rag/enhancement/cache.py)

```python
import hashlib
import json
import time
from typing import Any, Optional, Dict
import redis
from ...config.rag_config import rag_config

class RAGCache:
    """RAGç»“æœç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self):
        self.config = rag_config
        self.redis_client = None
        
        if self.config.ENABLE_CACHE:
            try:
                self.redis_client = redis.Redis(
                    host='localhost', 
                    port=6379, 
                    decode_responses=True
                )
                self.redis_client.ping()
            except:
                self.redis_client = None  # é™çº§ä¸ºå†…å­˜ç¼“å­˜
                self.memory_cache = {}
    
    def _generate_cache_key(self, query: str, context: Dict[str, Any] = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_data = {"query": query}
        if context:
            cache_data["context"] = context
        
        cache_string = json.dumps(cache_data, sort_keys=True, ensure_ascii=False)
        return f"rag:{hashlib.md5(cache_string.encode()).hexdigest()}"
    
    def get(self, query: str, context: Dict[str, Any] = None) -> Optional[Any]:
        """è·å–ç¼“å­˜ç»“æœ"""
        if not self.config.ENABLE_CACHE:
            return None
        
        cache_key = self._generate_cache_key(query, context)
        
        try:
            if self.redis_client:
                cached_data = self.redis_client.get(cache_key)
                if cached_data:
                    return json.loads(cached_data)
            else:
                # å†…å­˜ç¼“å­˜
                cached_item = self.memory_cache.get(cache_key)
                if cached_item and time.time() - cached_item['timestamp'] < self.config.CACHE_TTL:
                    return cached_item['data']
        except Exception:
            pass
        
        return None
    
    def set(self, query: str, result: Any, context: Dict[str, Any] = None):
        """è®¾ç½®ç¼“å­˜ç»“æœ"""
        if not self.config.ENABLE_CACHE:
            return
        
        cache_key = self._generate_cache_key(query, context)
        
        try:
            if self.redis_client:
                self.redis_client.setex(
                    cache_key, 
                    self.config.CACHE_TTL, 
                    json.dumps(result, ensure_ascii=False)
                )
            else:
                # å†…å­˜ç¼“å­˜
                self.memory_cache[cache_key] = {
                    'data': result,
                    'timestamp': time.time()
                }
                
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                current_time = time.time()
                expired_keys = [
                    k for k, v in self.memory_cache.items() 
                    if current_time - v['timestamp'] > self.config.CACHE_TTL
                ]
                for key in expired_keys:
                    del self.memory_cache[key]
                    
        except Exception:
            pass
```

## ğŸš€ éƒ¨ç½²ä¸ä½¿ç”¨

### 9. çŸ¥è¯†åº“åˆå§‹åŒ–è„šæœ¬ (scripts/init_knowledge_base.py)

```python
#!/usr/bin/env python3
"""çŸ¥è¯†åº“åˆå§‹åŒ–è„šæœ¬"""

import asyncio
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.rag.knowledge.manager import KnowledgeManager

async def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹åˆå§‹åŒ–çŸ¥è¯†åº“...")
    
    # åˆ›å»ºçŸ¥è¯†åº“ç®¡ç†å™¨
    manager = KnowledgeManager()
    
    # æ„å»ºçŸ¥è¯†åº“
    try:
        total_chunks = await manager.build_knowledge_base(force_rebuild=True)
        print(f"âœ… çŸ¥è¯†åº“åˆå§‹åŒ–æˆåŠŸï¼å…±å¤„ç† {total_chunks} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = manager.get_collection_stats()
        print(f"ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in stats.items():
            print(f"   {key}: {value}")
            
    except Exception as e:
        print(f"âŒ çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
```

### 10. ä½¿ç”¨ç¤ºä¾‹

```bash
# 1. å®‰è£…æ–°ä¾èµ–
pip install -r requirements.txt

# 2. å‡†å¤‡çŸ¥è¯†åº“æ–‡æ¡£
mkdir -p data/knowledge_base/{policies,procedures,faqs}

# 3. æ”¾ç½®æ–‡æ¡£åˆ°å¯¹åº”ç›®å½•
# - æ”¿ç­–æ–‡æ¡£ -> data/knowledge_base/policies/
# - æµç¨‹æ–‡æ¡£ -> data/knowledge_base/procedures/  
# - å¸¸è§é—®é¢˜ -> data/knowledge_base/faqs/

# 4. åˆå§‹åŒ–çŸ¥è¯†åº“
python scripts/init_knowledge_base.py

# 5. å¯åŠ¨åº”ç”¨ï¼ˆRAGåŠŸèƒ½è‡ªåŠ¨é›†æˆï¼‰
langgraph dev --port 2024
```

## ğŸ“ˆ æ•ˆæœè¯„ä¼°

### é¢„æœŸæ”¹è¿›æŒ‡æ ‡
- **å›ç­”å‡†ç¡®æ€§**: 70% â†’ 95%
- **æ”¿ç­–åˆè§„æ€§**: 60% â†’ 98%  
- **ç”¨æˆ·æ»¡æ„åº¦**: 3.5/5 â†’ 4.7/5
- **å¤„ç†æ•ˆç‡**: æå‡40%

### ç›‘æ§æŒ‡æ ‡
- æ£€ç´¢å“åº”æ—¶é—´ < 500ms
- çŸ¥è¯†åº“è¦†ç›–ç‡ > 90%
- ç¼“å­˜å‘½ä¸­ç‡ > 60%
- æ£€ç´¢ç›¸å…³æ€§ > 0.8

## ğŸ¯ æ€»ç»“

æ­¤RAGå®ç°æ–¹æ¡ˆæä¾›äº†ï¼š

1. **å®Œæ•´çš„çŸ¥è¯†ç®¡ç†ç³»ç»Ÿ** - æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼çš„è‡ªåŠ¨å¤„ç†
2. **é«˜æ•ˆçš„æ··åˆæ£€ç´¢** - ç»“åˆè¯­ä¹‰å’Œå…³é”®è¯æ£€ç´¢ä¼˜åŠ¿
3. **æ— ç¼çš„å·¥ä½œæµé›†æˆ** - ä¸ç°æœ‰LangGraphèŠ‚ç‚¹æ·±åº¦èåˆ
4. **çµæ´»çš„é…ç½®ç®¡ç†** - æ”¯æŒä¸ªæ€§åŒ–å‚æ•°è°ƒä¼˜
5. **å®Œå–„çš„ç¼“å­˜æœºåˆ¶** - æå‡ç³»ç»Ÿæ€§èƒ½å’Œç”¨æˆ·ä½“éªŒ

é€šè¿‡æ­¤æ–¹æ¡ˆï¼Œå·®æ—…æŠ¥é”€æ™ºèƒ½ä½“å°†å…·å¤‡å¼ºå¤§çš„çŸ¥è¯†æ¨ç†èƒ½åŠ›ï¼Œä¸ºç”¨æˆ·æä¾›æ›´å‡†ç¡®ã€æ›´åˆè§„çš„æœåŠ¡ä½“éªŒã€‚